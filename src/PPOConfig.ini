[training]
#----------------------------------------- Configuración del entorno -----------------------------------------#
# El entorno de OpenAI Gym en el que se entrenará el agente PPO.
env = ALE/Frogger-v5      
#----------------------------------------- Configuración del Agente -----------------------------------------#
# El tipo de agente de aprendizaje por refuerzo que se utilizará para entrenar en el entorno.
agent = PPO               
# La ruta del archivo donde se guardará el modelo entrenado.
model_path = ./models/PPOModel.pth   
#----------------------------------------- Configuración del entrenamiento -----------------------------------------#
# El número de episodios de entrenamiento que se deben completar antes de guardar el modelo entrenado en disco.
save_model_interval = 500  
# El número total de episodios de entrenamiento que se deben ejecutar. 
episodes = 20_000           
# La longitud máxima de cada episodio de entrenamiento en pasos. Si un episodio alcanza esta longitud, se termina automáticamente.
max_episode_length = 2_000
#----------------------------------------- Configuración del agente PPO -----------------------------------------#
# El valor inicial de la tasa de exploracion del agente.
eps_start = 0      
# El valor minimo de la tasa de exploracion del agente.
eps_end = 0     
# El número de pasos de entrenamiento que se deben realizar antes de que la tasa de exploracion del agente disminuya a su valor minimo.
eps_decay = 0
# La tasa de aprendizaje del optimizador utilizado para entrenar la red neuronal del agente. 
# Controla qué tan rápido se ajustan los pesos del modelo durante el entrenamiento.
learning_rate = 0.0001
# El factor de descuento utilizado para calcular los retornos descontados del agente. 
# Controla la importancia relativa de las recompensas futuras frente a las recompensas inmediatas.
gamma = 0.99
# El factor de descuento utilizado para calcular la ventaja (GAE - Generalized Advantage Estimation).
# Valores más altos reducen la varianza, valores más bajos reducen el sesgo.
gae_lambda = 0.95
# El parámetro de clipping utilizado para limitar cambios en la política durante una actualización.
# Previene cambios demasiado grandes en la política que podrían desestabilizar el entrenamiento.
clip_ratio = 0.1
# El número de pasos de optimización que se deben realizar en cada actualización de política.
# Más épocas de actualización pueden mejorar el aprendizaje pero también pueden causar overfitting.
epochs = 4
# El número de muestras de experiencia que se deben muestrear en cada paso de entrenamiento.
# Define el tamaño del lote de datos de experiencia recolectados antes de una actualización de política.
batch_size = 128
# El número de pasos de recolección de datos antes de realizar una actualización de política.
# Define cuánta experiencia se recolecta antes de entrenar el modelo.
n_steps = 2048
# El número de pasos de entrenamiento que se deben realizar antes de que la red neuronal del agente se actualicen con los pesos de la
# la red neuronal del agente. Esto controla con qué frecuencia se actualiza la red neuronal objetivo.
target_update = 0
# El número de transiciones de experiencia que se deben almacenar en la memoria de repeticion antes de que el
# agente comience a entrenar. Esto permite que el agente acumule un conjunto de experiencias inicial antes de 
# comenzar a entrenar.
initial_memory = 0
# El tamaño de la memoria de repeticion utilizada para almacenar las transiciones de experiencia del agente. 
# El agente entrena en muestras de experiencia muestreadas aleatoriamente de la memoria de repeticion.
memory_size = 0

[testing]
#----------------------------------------- Configuración del entorno -----------------------------------------#
# El entorno de OpenAI Gym en el que se probará el agente PPO.
env = ALE/Frogger-v5
#----------------------------------------- Configuración del entrenamiento -----------------------------------------#
# La longitud máxima de cada episodio de prueba en pasos. Si un episodio alcanza esta longitud, se termina automáticamente.
max_steps_per_episode = 2000   
#----------------------------------------- Configuración de output -----------------------------------------#
# El número total de episodios de prueba que se deben ejecutar.
episodes = 3                       
# La carpeta donde se guardarán los videos de las ejecuciones de prueba del agente.
video_folder = ./videos/PPOModel/   
#----------------------------------------- Configuración del Agente -----------------------------------------#
# El tipo de agente de aprendizaje por refuerzo que se utilizará para entrenar en el entorno.
agent = PPO               
# La ruta del archivo donde se encuentra el modelo entrenado que se utilizará para las pruebas.
model_path = ./models/PPOModel.pth
