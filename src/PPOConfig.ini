[training]
#----------------------------------------- Configuración del entorno -----------------------------------------#
# El entorno de OpenAI Gym en el que se entrenará el agente PPO.
env = ALE/Frogger-v5      
#----------------------------------------- Configuración del Agente -----------------------------------------#
# El tipo de agente de aprendizaje por refuerzo que se utilizará para entrenar en el entorno.
agent = PPO               
# La ruta del archivo donde se guardará el modelo entrenado.
model_path = ./models/PPOModel.pth   
# El nombre de la run de entrenamiento, utilizado para organizar los resultados.
run_name = PPOAgent
#----------------------------------------- Configuración del entrenamiento -----------------------------------------#
# El número de episodios de entrenamiento que se deben completar antes de guardar el modelo entrenado en disco.
save_model_interval = 500  
# El número total de episodios de entrenamiento que se deben ejecutar. 
episodes = 10_000           
# La longitud máxima de cada episodio de entrenamiento en pasos. Si un episodio alcanza esta longitud, se termina automáticamente.
max_episode_length = 2_000
#----------------------------------------- Configuración del agente PPO -----------------------------------------#
# La tasa de aprendizaje del optimizador utilizado para entrenar la red neuronal del agente. 
# Controla qué tan rápido se ajustan los pesos del modelo durante el entrenamiento.
learning_rate = 0.0001
# El factor de descuento utilizado para calcular los retornos descontados del agente. 
# Controla la importancia relativa de las recompensas futuras frente a las recompensas inmediatas.
gamma = 0.99
# El factor de descuento utilizado para calcular la ventaja (GAE - Generalized Advantage Estimation).
# Valores más altos reducen la varianza, valores más bajos reducen el sesgo.
gae_lambda = 0.95
# El parámetro de clipping utilizado para limitar cambios en la política durante una actualización.
# Previene cambios demasiado grandes en la política que podrían desestabilizar el entrenamiento.
clip_ratio = 0.1
# El número de pasos de optimización que se deben realizar en cada actualización de política.
# Más épocas de actualización pueden mejorar el aprendizaje pero también pueden causar overfitting.
epochs = 4
# El número de muestras de experiencia que se deben muestrear en cada paso de entrenamiento.
# Define el tamaño del lote de datos de experiencia recolectados antes de una actualización de política.
batch_size = 128
# El número de pasos de recolección de datos antes de realizar una actualización de política.
# Define cuánta experiencia se recolecta antes de entrenar el modelo.
n_steps = 2048

[testing]
#----------------------------------------- Configuración del entorno -----------------------------------------#
# El entorno de OpenAI Gym en el que se probará el agente PPO.
env = ALE/Frogger-v5
#----------------------------------------- Configuración del entrenamiento -----------------------------------------#
# La longitud máxima de cada episodio de prueba en pasos. Si un episodio alcanza esta longitud, se termina automáticamente.
max_steps_per_episode = 2000   
#----------------------------------------- Configuración de output -----------------------------------------#
# El número total de episodios de prueba que se deben ejecutar.
episodes = 3                       
# La carpeta donde se guardarán los videos de las ejecuciones de prueba del agente.
video_folder = ./videos/PPOModel/   
#----------------------------------------- Configuración del Agente -----------------------------------------#
# El tipo de agente de aprendizaje por refuerzo que se utilizará para entrenar en el entorno.
agent = PPO               
# La ruta del archivo donde se encuentra el modelo entrenado que se utilizará para las pruebas.
model_path = ./models/PPOModel.pth
